{"cells":[{"metadata":{},"cell_type":"markdown","source":["# Implementation of EfficientNetB5 for the APTOS 2019 competition with Keras"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"metadata":{},"cell_type":"markdown","source":["## Dependencies <a id=\"1\"></a>"]},{"metadata":{},"cell_type":"markdown","source":["Special thanks to [qubvel](https://github.com/qubvel/efficientnet) for sharing an amazing wrapper to get the EfficientNet architecture in one line of code!"]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":["import os\n","import sys\n","# Repository source: https://github.com/qubvel/efficientnet\n","import tensorflow \n","from tensorflow.python.keras.applications.efficientnet import EfficientNetB3\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","import glob\n","import re\n","from tensorflow.keras.backend import argmax\n","import numpy as np"],"execution_count":75,"outputs":[]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n"]},{"metadata":{},"cell_type":"markdown","source":["## Metric (Quadratic Weighted Kappa) <a id=\"3\"></a>"]},{"metadata":{},"cell_type":"markdown","source":["Batch Normalization becomes unstable with small batch sizes (<16) and that is why we use [Group Normalization ](https://arxiv.org/pdf/1803.08494.pdf) layers instead. Big thanks to [Somshubra Majumdar](https://github.com/titu1994) for building an implementation of Group Normalization for Keras.\n","\n","Keras makes it incredibly easy to replace layers. Just loop through the layers and replace each Batch Normalization layer with a Group Normalization layer."]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","import math as m\n","\n","# Repository source: https://github.com/qubvel/efficientnet\n","import tensorflow \n","from tensorflow.python.keras.applications.efficientnet import EfficientNetB3\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","import tensorflow as tf\n","def get_logits(labels, y_pred):\n","    s = 64.\n","    cos_t = y_pred\n","    sin_m = tf.math.sin(0.5)\n","    cos_m = tf.math.cos(0.5)\n","    cos_t2 = tf.square(cos_t, name='cos_2')\n","    sin_t2 = tf.subtract(1., cos_t2, name='sin_2')\n","    sin_t = tf.sqrt(sin_t2, name='sin_t')\n","    cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')\n","    threshold = tf.math.cos(tf.constant(m.pi) - 0.5)\n","    cond_v = cos_t - threshold\n","    cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)\n","    mm = sin_m * 0.5\n","    keep_val = s*(cos_t - mm)\n","    cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n","    mask = tf.one_hot(labels, depth=2, name='one_hot_mask')\n","    inv_mask = tf.subtract(1., mask, name='inverse_mask')\n","    s_cos_t = tf.multiply(s, cos_t, name='scalar_cos_t')\n","    output = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_logits')\n","    return output\n","def loss(y_true, y_pred):\n","    labels = argmax(y_true, axis=-1)\n","    logits = get_logits(labels, y_pred)\n","    #tf.keras.losses.SparseCategoricalCrossentropy()\n","    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n","    return loss"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["effnet = EfficientNetB3(weights = \"imagenet\", include_top = False, input_shape=(299,299,3), classes = 2)\n"]},{"cell_type":"code","execution_count":66,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":["____________\nblock6b_project_bn (BatchNormal (None, 10, 10, 232)  928         block6b_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock6b_drop (Dropout)          (None, 10, 10, 232)  0           block6b_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6b_add (Add)               (None, 10, 10, 232)  0           block6b_drop[0][0]               \n                                                                 block6a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6c_expand_conv (Conv2D)    (None, 10, 10, 1392) 322944      block6b_add[0][0]                \n__________________________________________________________________________________________________\nblock6c_expand_bn (BatchNormali (None, 10, 10, 1392) 5568        block6c_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock6c_expand_activation (Acti (None, 10, 10, 1392) 0           block6c_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock6c_dwconv (DepthwiseConv2D (None, 10, 10, 1392) 34800       block6c_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock6c_bn (BatchNormalization) (None, 10, 10, 1392) 5568        block6c_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock6c_activation (Activation) (None, 10, 10, 1392) 0           block6c_bn[0][0]                 \n__________________________________________________________________________________________________\nblock6c_se_squeeze (GlobalAvera (None, 1392)         0           block6c_activation[0][0]         \n__________________________________________________________________________________________________\nblock6c_se_reshape (Reshape)    (None, 1, 1, 1392)   0           block6c_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock6c_se_reduce (Conv2D)      (None, 1, 1, 58)     80794       block6c_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock6c_se_expand (Conv2D)      (None, 1, 1, 1392)   82128       block6c_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock6c_se_excite (Multiply)    (None, 10, 10, 1392) 0           block6c_activation[0][0]         \n                                                                 block6c_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock6c_project_conv (Conv2D)   (None, 10, 10, 232)  322944      block6c_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock6c_project_bn (BatchNormal (None, 10, 10, 232)  928         block6c_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock6c_drop (Dropout)          (None, 10, 10, 232)  0           block6c_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6c_add (Add)               (None, 10, 10, 232)  0           block6c_drop[0][0]               \n                                                                 block6b_add[0][0]                \n__________________________________________________________________________________________________\nblock6d_expand_conv (Conv2D)    (None, 10, 10, 1392) 322944      block6c_add[0][0]                \n__________________________________________________________________________________________________\nblock6d_expand_bn (BatchNormali (None, 10, 10, 1392) 5568        block6d_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock6d_expand_activation (Acti (None, 10, 10, 1392) 0           block6d_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock6d_dwconv (DepthwiseConv2D (None, 10, 10, 1392) 34800       block6d_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock6d_bn (BatchNormalization) (None, 10, 10, 1392) 5568        block6d_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock6d_activation (Activation) (None, 10, 10, 1392) 0           block6d_bn[0][0]                 \n__________________________________________________________________________________________________\nblock6d_se_squeeze (GlobalAvera (None, 1392)         0           block6d_activation[0][0]         \n__________________________________________________________________________________________________\nblock6d_se_reshape (Reshape)    (None, 1, 1, 1392)   0           block6d_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock6d_se_reduce (Conv2D)      (None, 1, 1, 58)     80794       block6d_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock6d_se_expand (Conv2D)      (None, 1, 1, 1392)   82128       block6d_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock6d_se_excite (Multiply)    (None, 10, 10, 1392) 0           block6d_activation[0][0]         \n                                                                 block6d_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock6d_project_conv (Conv2D)   (None, 10, 10, 232)  322944      block6d_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock6d_project_bn (BatchNormal (None, 10, 10, 232)  928         block6d_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock6d_drop (Dropout)          (None, 10, 10, 232)  0           block6d_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6d_add (Add)               (None, 10, 10, 232)  0           block6d_drop[0][0]               \n                                                                 block6c_add[0][0]                \n__________________________________________________________________________________________________\nblock6e_expand_conv (Conv2D)    (None, 10, 10, 1392) 322944      block6d_add[0][0]                \n__________________________________________________________________________________________________\nblock6e_expand_bn (BatchNormali (None, 10, 10, 1392) 5568        block6e_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock6e_expand_activation (Acti (None, 10, 10, 1392) 0           block6e_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock6e_dwconv (DepthwiseConv2D (None, 10, 10, 1392) 34800       block6e_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock6e_bn (BatchNormalization) (None, 10, 10, 1392) 5568        block6e_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock6e_activation (Activation) (None, 10, 10, 1392) 0           block6e_bn[0][0]                 \n__________________________________________________________________________________________________\nblock6e_se_squeeze (GlobalAvera (None, 1392)         0           block6e_activation[0][0]         \n__________________________________________________________________________________________________\nblock6e_se_reshape (Reshape)    (None, 1, 1, 1392)   0           block6e_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock6e_se_reduce (Conv2D)      (None, 1, 1, 58)     80794       block6e_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock6e_se_expand (Conv2D)      (None, 1, 1, 1392)   82128       block6e_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock6e_se_excite (Multiply)    (None, 10, 10, 1392) 0           block6e_activation[0][0]         \n                                                                 block6e_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock6e_project_conv (Conv2D)   (None, 10, 10, 232)  322944      block6e_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock6e_project_bn (BatchNormal (None, 10, 10, 232)  928         block6e_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock6e_drop (Dropout)          (None, 10, 10, 232)  0           block6e_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6e_add (Add)               (None, 10, 10, 232)  0           block6e_drop[0][0]               \n                                                                 block6d_add[0][0]                \n__________________________________________________________________________________________________\nblock6f_expand_conv (Conv2D)    (None, 10, 10, 1392) 322944      block6e_add[0][0]                \n__________________________________________________________________________________________________\nblock6f_expand_bn (BatchNormali (None, 10, 10, 1392) 5568        block6f_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock6f_expand_activation (Acti (None, 10, 10, 1392) 0           block6f_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock6f_dwconv (DepthwiseConv2D (None, 10, 10, 1392) 34800       block6f_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock6f_bn (BatchNormalization) (None, 10, 10, 1392) 5568        block6f_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock6f_activation (Activation) (None, 10, 10, 1392) 0           block6f_bn[0][0]                 \n__________________________________________________________________________________________________\nblock6f_se_squeeze (GlobalAvera (None, 1392)         0           block6f_activation[0][0]         \n__________________________________________________________________________________________________\nblock6f_se_reshape (Reshape)    (None, 1, 1, 1392)   0           block6f_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock6f_se_reduce (Conv2D)      (None, 1, 1, 58)     80794       block6f_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock6f_se_expand (Conv2D)      (None, 1, 1, 1392)   82128       block6f_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock6f_se_excite (Multiply)    (None, 10, 10, 1392) 0           block6f_activation[0][0]         \n                                                                 block6f_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock6f_project_conv (Conv2D)   (None, 10, 10, 232)  322944      block6f_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock6f_project_bn (BatchNormal (None, 10, 10, 232)  928         block6f_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock6f_drop (Dropout)          (None, 10, 10, 232)  0           block6f_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock6f_add (Add)               (None, 10, 10, 232)  0           block6f_drop[0][0]               \n                                                                 block6e_add[0][0]                \n__________________________________________________________________________________________________\nblock7a_expand_conv (Conv2D)    (None, 10, 10, 1392) 322944      block6f_add[0][0]                \n__________________________________________________________________________________________________\nblock7a_expand_bn (BatchNormali (None, 10, 10, 1392) 5568        block7a_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock7a_expand_activation (Acti (None, 10, 10, 1392) 0           block7a_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock7a_dwconv (DepthwiseConv2D (None, 10, 10, 1392) 12528       block7a_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock7a_bn (BatchNormalization) (None, 10, 10, 1392) 5568        block7a_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock7a_activation (Activation) (None, 10, 10, 1392) 0           block7a_bn[0][0]                 \n__________________________________________________________________________________________________\nblock7a_se_squeeze (GlobalAvera (None, 1392)         0           block7a_activation[0][0]         \n__________________________________________________________________________________________________\nblock7a_se_reshape (Reshape)    (None, 1, 1, 1392)   0           block7a_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock7a_se_reduce (Conv2D)      (None, 1, 1, 58)     80794       block7a_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock7a_se_expand (Conv2D)      (None, 1, 1, 1392)   82128       block7a_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock7a_se_excite (Multiply)    (None, 10, 10, 1392) 0           block7a_activation[0][0]         \n                                                                 block7a_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock7a_project_conv (Conv2D)   (None, 10, 10, 384)  534528      block7a_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock7a_project_bn (BatchNormal (None, 10, 10, 384)  1536        block7a_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock7b_expand_conv (Conv2D)    (None, 10, 10, 2304) 884736      block7a_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock7b_expand_bn (BatchNormali (None, 10, 10, 2304) 9216        block7b_expand_conv[0][0]        \n__________________________________________________________________________________________________\nblock7b_expand_activation (Acti (None, 10, 10, 2304) 0           block7b_expand_bn[0][0]          \n__________________________________________________________________________________________________\nblock7b_dwconv (DepthwiseConv2D (None, 10, 10, 2304) 20736       block7b_expand_activation[0][0]  \n__________________________________________________________________________________________________\nblock7b_bn (BatchNormalization) (None, 10, 10, 2304) 9216        block7b_dwconv[0][0]             \n__________________________________________________________________________________________________\nblock7b_activation (Activation) (None, 10, 10, 2304) 0           block7b_bn[0][0]                 \n__________________________________________________________________________________________________\nblock7b_se_squeeze (GlobalAvera (None, 2304)         0           block7b_activation[0][0]         \n__________________________________________________________________________________________________\nblock7b_se_reshape (Reshape)    (None, 1, 1, 2304)   0           block7b_se_squeeze[0][0]         \n__________________________________________________________________________________________________\nblock7b_se_reduce (Conv2D)      (None, 1, 1, 96)     221280      block7b_se_reshape[0][0]         \n__________________________________________________________________________________________________\nblock7b_se_expand (Conv2D)      (None, 1, 1, 2304)   223488      block7b_se_reduce[0][0]          \n__________________________________________________________________________________________________\nblock7b_se_excite (Multiply)    (None, 10, 10, 2304) 0           block7b_activation[0][0]         \n                                                                 block7b_se_expand[0][0]          \n__________________________________________________________________________________________________\nblock7b_project_conv (Conv2D)   (None, 10, 10, 384)  884736      block7b_se_excite[0][0]          \n__________________________________________________________________________________________________\nblock7b_project_bn (BatchNormal (None, 10, 10, 384)  1536        block7b_project_conv[0][0]       \n__________________________________________________________________________________________________\nblock7b_drop (Dropout)          (None, 10, 10, 384)  0           block7b_project_bn[0][0]         \n__________________________________________________________________________________________________\nblock7b_add (Add)               (None, 10, 10, 384)  0           block7b_drop[0][0]               \n                                                                 block7a_project_bn[0][0]         \n__________________________________________________________________________________________________\ntop_conv (Conv2D)               (None, 10, 10, 1536) 589824      block7b_add[0][0]                \n__________________________________________________________________________________________________\ntop_bn (BatchNormalization)     (None, 10, 10, 1536) 6144        top_conv[0][0]                   \n__________________________________________________________________________________________________\ntop_activation (Activation)     (None, 10, 10, 1536) 0           top_bn[0][0]                     \n==================================================================================================\nTotal params: 10,783,535\nTrainable params: 10,696,232\nNon-trainable params: 87,303\n__________________________________________________________________________________________________\nNone\n"]}],"source":["effnet.compile(loss=loss,\n","                      optimizer=Adam(lr=0.0005), \n","                  metrics=['acc'])\n","print(effnet.summary())"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["DATASET_PATHS = {\n","    'youtube': 'original_sequences/youtube/c23/',\n","    'actors': 'original_sequences/actors/c23/',\n","    'Deepfakes': 'manipulated_sequences/Deepfakes/c23/',\n","    'Face2Face': 'manipulated_sequences/Face2Face/c23/',\n","    'FaceShifter': 'manipulated_sequences/FaceShifter/c23/',\n","    'FaceSwap': 'manipulated_sequences/FaceSwap/c23/',\n","    'deepfakedetection': 'manipulated_sequences/DeepFakeDetection/c23/'\n","}\n","data_path = \"/Users/asmaaaly/Minerva/Capstone/Minerva_Capstone/Capstone/data/\"\n","output_path = \"/Users/asmaaaly/Minerva/Capstone/Minerva_Capstone/Capstone/data/\"\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["data = pd.read_csv(\"/Users/asmaaaly/Minerva/Minerva_Capstone/1.Preprocess Data/data_csv.csv\")"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def read_images(path):\n","    \"\"\"\n","    read_images iterates through the preprocessed\n","    images and adds the labels for each image\n","    based on the directory of the image\n","    :param path (str): path for the preprocessed\n","    image\n","    :param dataset (str): dataset name\n","    :return: a Pandas dataframe with \n","    image label and .\n","    \"\"\" \n","    data = []\n","    filenames = glob.glob(Processed_datasets[path])\n","    for image in filenames:\n","        frame_name = image.split(\"/\")[-1]\n","        dataset = frame_name.split(\"_\")[0]\n","        if dataset == \"Face2Face\":\n","            video_name = re.findall(r'\\d+',image)[1]\n","        else: \n","            video_name = re.findall(r'\\d+',image)[0]\n","        if path == 'Real':\n","            label = 0\n","            data.append([video_name, label,frame_name,dataset])\n","        else: \n","            label = 1\n","            data.append([video_name, label,frame_name,dataset])\n","    return data"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Processed_datasets = {\\n    'Real': '/Volumes/MY PASSPORT/Base_directory/Real/*.jpg',\\n    'Fake': '/Volumes/MY PASSPORT/Base_directory/Fake/*.jpg'\\n}\""]},"metadata":{},"execution_count":27}],"source":["\"\"\"Processed_datasets = {\n","    'Real': '/Volumes/MY PASSPORT/Base_directory/Real/*.jpg',\n","    'Fake': '/Volumes/MY PASSPORT/Base_directory/Fake/*.jpg'\n","}\"\"\""]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[],"source":["training_dataset = {\n","    'Real': '/Volumes/MY PASSPORT/Base_directory/Real/',\n","    'Fake': '/Volumes/MY PASSPORT/Base_directory/Fake/'\n","}\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'data_real = pd.DataFrame(read_images(\\'Real\\'),columns=[\"Video_ID\",\"Label\",\"Frame_ID\",\"Dataset\"])\\ndata_fake = pd.DataFrame(read_images(\\'Fake\\'),columns=[\"Video_ID\",\"Label\",\"Frame_ID\",\"Dataset\"])\\n#Combine both datframes\\nframes = [data_real, data_fake]\\ndf_data = pd.concat(frames)'"]},"metadata":{},"execution_count":29}],"source":["\"\"\"data_real = pd.DataFrame(read_images('Real'),columns=[\"Video_ID\",\"Label\",\"Frame_ID\",\"Dataset\"])\n","data_fake = pd.DataFrame(read_images('Fake'),columns=[\"Video_ID\",\"Label\",\"Frame_ID\",\"Dataset\"])\n","#Combine both datframes\n","frames = [data_real, data_fake]\n","df_data = pd.concat(frames)\"\"\""]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["#this is the df that has all of the images, will be used for mapping\n","df = pd.read_csv(\"/Users/asmaaaly/Minerva/Minerva_Capstone/1.Preprocess Data/all_data_pd.csv\",index_col=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["            Video_ID  Frame_ID\n","0       000Deepfakes       101\n","1         000youtube       101\n","2       001Deepfakes       101\n","3       001Face2Face       101\n","4     001FaceShifter       101\n","...              ...       ...\n","3404     998FaceSwap       101\n","3405      998youtube       102\n","3406    999Deepfakes       101\n","3407     999FaceSwap       101\n","3408      999youtube       101\n","\n","[3409 rows x 2 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Video_ID</th>\n      <th>Frame_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000Deepfakes</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000youtube</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001Deepfakes</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001Face2Face</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>001FaceShifter</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3404</th>\n      <td>998FaceSwap</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>3405</th>\n      <td>998youtube</td>\n      <td>102</td>\n    </tr>\n    <tr>\n      <th>3406</th>\n      <td>999Deepfakes</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>3407</th>\n      <td>999FaceSwap</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>3408</th>\n      <td>999youtube</td>\n      <td>101</td>\n    </tr>\n  </tbody>\n</table>\n<p>3409 rows × 2 columns</p>\n</div>"},"metadata":{},"execution_count":21}],"source":["df_data = df.groupby('Video_ID')[[\"Frame_ID\"]].agg(\"count\").reset_index()\n","df_data"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["labels = df[[\"Video_ID\",\"Label\"]]"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["merged_df_video_only = df_data.merge(labels, how = 'inner', on = ['Video_ID']).drop_duplicates(subset=['Video_ID'])\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["merged_df_video_only = merged_df_video_only[~(merged_df_video_only['Frame_ID'] <= 100)] "]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              Video_ID  Frame_ID  Label\n","0         000Deepfakes       101      1\n","101         000youtube       101      0\n","202       001Deepfakes       101      1\n","303       001Face2Face       101      1\n","404     001FaceShifter       101      1\n","...                ...       ...    ...\n","323286     998FaceSwap       101      1\n","323387      998youtube       102      0\n","323489    999Deepfakes       101      1\n","323590     999FaceSwap       101      1\n","323691      999youtube       101      0\n","\n","[3098 rows x 3 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Video_ID</th>\n      <th>Frame_ID</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000Deepfakes</td>\n      <td>101</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>000youtube</td>\n      <td>101</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>001Deepfakes</td>\n      <td>101</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>303</th>\n      <td>001Face2Face</td>\n      <td>101</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>404</th>\n      <td>001FaceShifter</td>\n      <td>101</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>323286</th>\n      <td>998FaceSwap</td>\n      <td>101</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>323387</th>\n      <td>998youtube</td>\n      <td>102</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>323489</th>\n      <td>999Deepfakes</td>\n      <td>101</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>323590</th>\n      <td>999FaceSwap</td>\n      <td>101</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>323691</th>\n      <td>999youtube</td>\n      <td>101</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3098 rows × 3 columns</p>\n</div>"},"metadata":{},"execution_count":34}],"source":["merged_df_video_only"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"df_0 = merged_df_video_only[merged_df_video_only['Label'] == 0].sample(1166, random_state = 101)\\ndf_1 = merged_df_video_only[merged_df_video_only['Label'] == 1].sample(1166, random_state = 101)\""]},"metadata":{},"execution_count":36}],"source":["\"\"\"df_0 = merged_df_video_only[merged_df_video_only['Label'] == 0].sample(1166, random_state = 101)\n","df_1 = merged_df_video_only[merged_df_video_only['Label'] == 1].sample(1166, random_state = 101)\"\"\""]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"EOL while scanning string literal (<ipython-input-37-2a1dd404ed8a>, line 4)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-2a1dd404ed8a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    df_data['Label'].value_counts()\"\"\"\"\u001b[0m\n\u001b[0m                                       \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"]}],"source":["\"\"\"\"df_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n","# shuffle\n","#print to see if the data is balanced \n","df_data['Label'].value_counts()\"\"\"\""]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["\"\"\"\"df_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n","# shuffle\n","#print to see if the data is balanced \n","df_data['Label'].value_counts()\"\"\"\"\n","\n","\"\"\"y = df_data['Label']\n","df_train, df_test = train_test_split(df_data, test_size=0.20, random_state=101, stratify=y)\n","\"\"\"\n","\"\"\"y_train = df_train['Label']\n","df_train, df_valid = train_test_split(df_train, test_size=0.25, random_state=101,stratify=y_train)\"\"\"\n","\"\"\"df_train[\"Split Type\"] = np.asarray(['Train'] * len(df_train))\n","df_valid[\"Split Type\"] = np.asarray(['Valid'] * len(df_valid))\n","df_test[\"Split Type\"] = np.asarray(['Test'] * len(df_test))\n","Processed_frames = [df_train, df_valid,df_test]\n","df_data_used = pd.concat(Processed_frames)\"\"\"\n"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                             Video_ID  Number_frames  Label Split_type\n","746                        597youtube            101      0      Train\n","336                        569youtube            102      0      Train\n","2209                      267FaceSwap            101      1      Train\n","176                        350youtube            101      0      Train\n","1024                       978youtube            101      0      Train\n","...                               ...            ...    ...        ...\n","2116                      103FaceSwap            101      1       Test\n","1594                     200Deepfakes            101      1       Test\n","960   04__talking_against_wall_actors            101      0       Test\n","1929                   107FaceShifter            101      1       Test\n","609                        768youtube            101      0       Test\n","\n","[2332 rows x 4 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Video_ID</th>\n      <th>Number_frames</th>\n      <th>Label</th>\n      <th>Split_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>746</th>\n      <td>597youtube</td>\n      <td>101</td>\n      <td>0</td>\n      <td>Train</td>\n    </tr>\n    <tr>\n      <th>336</th>\n      <td>569youtube</td>\n      <td>102</td>\n      <td>0</td>\n      <td>Train</td>\n    </tr>\n    <tr>\n      <th>2209</th>\n      <td>267FaceSwap</td>\n      <td>101</td>\n      <td>1</td>\n      <td>Train</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>350youtube</td>\n      <td>101</td>\n      <td>0</td>\n      <td>Train</td>\n    </tr>\n    <tr>\n      <th>1024</th>\n      <td>978youtube</td>\n      <td>101</td>\n      <td>0</td>\n      <td>Train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2116</th>\n      <td>103FaceSwap</td>\n      <td>101</td>\n      <td>1</td>\n      <td>Test</td>\n    </tr>\n    <tr>\n      <th>1594</th>\n      <td>200Deepfakes</td>\n      <td>101</td>\n      <td>1</td>\n      <td>Test</td>\n    </tr>\n    <tr>\n      <th>960</th>\n      <td>04__talking_against_wall_actors</td>\n      <td>101</td>\n      <td>0</td>\n      <td>Test</td>\n    </tr>\n    <tr>\n      <th>1929</th>\n      <td>107FaceShifter</td>\n      <td>101</td>\n      <td>1</td>\n      <td>Test</td>\n    </tr>\n    <tr>\n      <th>609</th>\n      <td>768youtube</td>\n      <td>101</td>\n      <td>0</td>\n      <td>Test</td>\n    </tr>\n  </tbody>\n</table>\n<p>2332 rows × 4 columns</p>\n</div>"},"metadata":{},"execution_count":57}],"source":["\"\"\"df_data_used.columns = ['Video_ID', 'Number_frames','Label',\"Split_type\"]\n","df_data_used\"\"\""]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["\"\"\"#save the datat preprocessed as csv file\n","data_with_nframes = df_data_used.merge(df, how = 'inner', on = ['Video_ID'])\n","\n","data_full =data_with_nframes\"\"\""]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["#data_full.to_csv(\"data_full.csv\",index=False)"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["data_full = pd.read_csv(\"data_full.csv\")"]},{"source":["#data_full = data_full.drop(\"Label_y\",axis=1)\n"],"cell_type":"code","metadata":{},"execution_count":59,"outputs":[]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["\"\"\"data_full.columns = ['Video_ID', 'Number_frames','Label',\"Split_type\",\"Frame_ID\",\"Dataset\"]\"\"\""]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["\"\"\"data_with_nframes.to_csv(\"data_with_nframes.csv\",header=False)\"\"\""]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["data_with_nframes = pd.read_csv(\"/Users/asmaaaly/Minerva/Minerva_Capstone/2.Models/data_with_nframes.csv\")"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["from keras.preprocessing.image import img_to_array, load_img\n","import numpy as np\n","\n","def process_image(image, target_shape):\n","    \"\"\"Given an image, process it and return the array.\"\"\"\n","    # Load the image.\n","    h, w, _ = target_shape\n","    print(image)\n","    image = load_img(image, target_size=(h, w))\n","\n","    # Turn it into numpy, normalize and return.\n","    img_arr = img_to_array(image)\n","    x = (img_arr / 255.).astype(np.float32)\n","\n","    return x\n"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]}],"source":["\"\"\"\n","Class for managing our data.\n","\"\"\"\n","import csv\n","import numpy as np\n","import random\n","import glob\n","import os.path\n","import sys\n","import operator\n","import threading\n","from tensorflow.keras.models import Model, load_model\n","\n","\n","from keras.utils import to_categorical\n","\n","class threadsafe_iterator:\n","    def __init__(self, iterator):\n","        self.iterator = iterator\n","        self.lock = threading.Lock()\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        with self.lock:\n","            return next(self.iterator)\n","\n","def threadsafe_generator(func):\n","    \"\"\"Decorator\"\"\"\n","    def gen(*a, **kw):\n","        return threadsafe_iterator(func(*a, **kw))\n","    return gen\n","\n","class DataSet():\n","\n","    def __init__(self, seq_length=100, class_limit=None, image_shape=(224, 224, 3)):\n","        \"\"\"Constructor.\n","        seq_length = (int) the number of frames to consider\n","        class_limit = (int) number of classes to limit the data to.\n","            None = no limit.\n","        \"\"\"\n","        self.seq_length = seq_length\n","        self.class_limit = class_limit\n","        self.sequence_path = os.path.join('data', 'sequences')\n","        self.max_frames = 300  # max number of frames a video can have for us to use it\n","\n","        # Get the data.\n","        self.data = self.get_data()\n","\n","        # Get the classes.\n","        self.classes = self.get_classes()\n","\n","        # Now do some minor data cleaning.\n","        #self.data = self.clean_data()\n","\n","        self.image_shape = image_shape\n","\n","    @staticmethod\n","    def get_data():\n","        \"\"\"Load our data from file.\"\"\"\n","        with open(os.path.join('/Users/asmaaaly/Minerva/Minerva_Capstone/1.Preprocess Data', 'only_videos_with_labels.csv'), 'r') as fin:\n","            reader = csv.reader(fin)\n","            data = list(reader)\n","\n","        return data\n","\n","    def clean_data(self):\n","        \"\"\"Limit samples to greater than the sequence length and fewer\n","        than N frames. Also limit it to classes we want to use.\"\"\"\n","        data_clean = []\n","        for item in self.data:\n","            if int(item[1]) >= self.seq_length and int(item[1]) <= self.max_frames \\\n","                    and item[2] in self.classes:\n","                data_clean.append(item)\n","\n","        return data_clean\n","\n","    def get_classes(self):\n","        \"\"\"Extract the classes from our data. If we want to limit them,\n","        only return the classes we need.\"\"\"\n","        classes = []\n","        for item in self.data:\n","            if item[3] not in classes:\n","                classes.append(item[3])\n","\n","        # Sort them.\n","        classes = sorted(classes)\n","\n","        # Return.\n","        if self.class_limit is not None:\n","            return classes[:self.class_limit]\n","        else:\n","            return classes\n","\n","    def get_class_one_hot(self, class_str):\n","        \"\"\"Given a class as a string, return its number in the classes\n","        list. This lets us encode and one-hot it for training.\"\"\"\n","        # Encode it first.\n","        #label_encoded = self.classes.index(class_str)\n","\n","        # Now one-hot it.\n","        #label_hot = to_categorical(class_str, len(self.classes))\n","        label_hot = float(class_str)\n","        #assert len(label_hot) == len(self.classes)\n","\n","        return label_hot\n","\n","    def split_train_test(self):\n","        \"\"\"Split the data into train and test groups.\"\"\"\n","        train = []\n","        test = []\n","        valid = []\n","        for item in self.data:\n","            if item[4] == 'Train':\n","                train.append(item)\n","            elif item[4] == 'Valid':\n","                valid.append(item)\n","            else:\n","                test.append(item)\n","        return train, valid, test\n","\n","    def get_all_sequences_in_memory(self, train_test, data_type):\n","        \"\"\"\n","        This is a mirror of our generator, but attempts to load everything into\n","        memory so we can train way faster.\n","        \"\"\"\n","        # Get the right dataset.\n","        train, valid, test = self.split_train_test()\n","        if train_test == 'train':\n","            data = train \n","        elif train_test == 'valid':\n","            data = valid\n","        else:\n","            data = test \n","\n","        print(\"Loading %d samples into memory for %sing.\" % (len(data), train_test))\n","\n","        X, y = [], []\n","        for row in data:\n","            if data_type == 'images':\n","                frames = self.get_frames_for_sample(row)\n","                frames = self.rescale_list(frames, self.seq_length)\n","\n","                # Build the image sequence\n","                sequence = self.build_image_sequence(frames)\n","\n","            else:\n","                sequence = self.get_extracted_sequence(data_type, row)\n","\n","                if sequence is None:\n","                    print(\"Can't find sequence. Did you generate them?\")\n","                    raise\n","\n","            X.append(sequence)\n","            y.append(row[3])\n","\n","        return np.array(X), np.array(y)\n","\n","    @threadsafe_generator\n","    def frame_generator(self, batch_size, train_test, data_type):\n","        \"\"\"Return a generator that we can use to train on. There are\n","        a couple different things we can return:\n","\n","        data_type: 'features', 'images'\n","        \"\"\"\n","        # Get the right dataset for the generator.\n","        train, valid, test = self.split_train_test()\n","        if train_test == 'train':\n","            data = train\n","        elif train_test == 'valid':\n","            data = valid\n","        else:\n","            data = test\n","        print(\"Creating %s generator with %d samples.\" % (train_test, len(data)))\n","\n","        while 1:\n","            X, y = [], []\n","\n","            # Generate batch_size samples.\n","            for _ in range(batch_size):\n","                # Reset to be safe.\n","                sequence = None\n","                # Get a random sample.\n","                sample = random.choice(data)\n","\n","                # Check to see if we've already saved this sequence.\n","                if data_type is \"images\":\n","                    # Get and resample frames.\n","                    frames = self.get_frames_for_sample(sample)\n","                    frames = self.rescale_list(frames, self.seq_length)\n","\n","                    # Build the image sequence\n","                    sequence = self.build_image_sequence(frames)\n","                else:\n","                    # Get the sequence from disk.\n","                    sequence = self.get_extracted_sequence(data_type, sample)\n","\n","                    if sequence is None:\n","                        raise ValueError(\"Can't find sequence. Did you generate them?\")\n","\n","                X.append(sequence)\n","                y.append(self.get_class_one_hot(sample[3]))\n","            yield np.array(X), np.array(y)\n","    def build_image_sequence(self, frames):\n","        \"\"\"Given a set of frames (filenames), build our sequence.\"\"\"\n","        return [process_image(x, self.image_shape) for x in frames]\n","\n","    def get_extracted_sequence(self, data_type, sample):\n","        \"\"\"Get the saved extracted features.\"\"\"\n","        filename = sample[1]\n","        path = os.path.join(data_path, \"sequences\",filename + '-' + str(self.seq_length) + \\\n","            '-' + data_type + '.npy')\n","        if os.path.isfile(path):\n","            return np.load(path)\n","        else:\n","            return None\n","\n","    def get_frames_by_filename(self, filename, data_type):\n","        \"\"\"Given a filename for one of our samples, return the data\n","        the model needs to make predictions.\"\"\"\n","        # First, find the sample row.\n","        sample = None\n","        for row in self.data:\n","            if row[1] == filename:\n","                sample = row\n","                break\n","        if sample is None:\n","            raise ValueError(\"Couldn't find sample: %s\" % filename)\n","\n","        if data_type == \"images\":\n","            # Get and resample frames.\n","            frames = self.get_frames_for_sample(sample)\n","            frames = self.rescale_list(frames, self.seq_length)\n","            # Build the image sequence\n","            sequence = self.build_image_sequence(frames)\n","        else:\n","            # Get the sequence from disk.\n","            sequence = self.get_extracted_sequence(data_type, sample)\n","\n","            if sequence is None:\n","                raise ValueError(\"Can't find sequence. Did you generate them?\")\n","\n","        return sequence\n","\n","    @staticmethod\n","    def get_frames_for_sample(sample):\n","        sample = sample[1]\n","        sources = []\n","        images = data_full.loc[data_full['Video_ID'] == sample]['Frame_ID']\n","        target = data_full.loc[data_full['Video_ID'] == sample]['Label']\n","        if target.any() == 0:\n","            for i in images:\n","                src = os.path.join(training_dataset['Real'], i)\n","                sources.append(src)\n","        else: \n","            for i in images:\n","                src = os.path.join(training_dataset['Fake'], i)\n","                sources.append(src)\n","        return sources\n","\n","    @staticmethod\n","    def get_filename_from_image(filename):\n","        parts = filename.split(os.path.sep)\n","        return parts[-1].replace('.jpg', '')\n","\n","    @staticmethod\n","    def rescale_list(input_list, size):\n","        \"\"\"Given a list and a size, return a rescaled/samples list. For example,\n","        if we want a list of size 5 and we have a list of size 25, return a new\n","        list of size five which is every 5th element of the origina list.\"\"\"\n","        assert len(input_list) >= size\n","\n","        # Get the number to skip between iterations.\n","        skip = len(input_list) // size\n","\n","        # Build our new output.\n","        output = [input_list[i] for i in range(0, len(input_list), skip)]\n","\n","        # Cut off the last one if needed.\n","        return output[:size]\n","\n","    def print_class_from_prediction(self, predictions, nb_to_return=5):\n","        \"\"\"Given a prediction, print the top classes.\"\"\"\n","        # Get the prediction for each label.\n","        label_predictions = {}\n","        for i, label in enumerate(self.classes):\n","            label_predictions[label] = predictions[i]\n","\n","        # Now sort them.\n","        sorted_lps = sorted(\n","            label_predictions.items(),\n","            key=operator.itemgetter(1),\n","            reverse=True\n","        )\n","\n","        # And return the top N.\n","        for i, class_prediction in enumerate(sorted_lps):\n","            if i > nb_to_return - 1 or class_prediction[1] == 0.0:\n","                break\n","            print(\"%s: %.2f\" % (class_prediction[0], class_prediction[1]))\n"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.efficientnet import preprocess_input\n","#from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input\n","import numpy as np\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input\n","import numpy as np"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x7fe581a9ac10>"]},"metadata":{},"execution_count":24}],"source":["effnet.layers[-12]"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["from tensorflow.keras.preprocessing import image\n","\n","class Extractor():\n","    def __init__(self, weights=\"None\"):\n","        \"\"\"Either load pretrained from imagenet, or load our saved\n","        weights from our own training.\"\"\"\n","\n","        self.weights = weights  # so we can check elsewhere which model\n","\n","        if weights is None:\n","            # We'll extract features at the final pool layer.\n","            base_model = effnet\n","\n","            # We'll extract features at the final pool layer.\n","            self.extractor_model = Model(\n","                inputs=base_model.input,\n","                outputs=base_model.layers[-12].output\n","            )\n","        elif self.weights == \"effec\":\n","            # Load the model first.\n","            self.extractor_model = model_2\n","\n","            # Then remove the top so we get features not predictions.\n","            # From: https://github.com/fchollet/keras/issues/2371\n","            self.extractor_model.layers.pop()\n","            self.extractor_model.layers.pop()  # two pops to get to pool layer\n","            self.extractor_model.outputs = [self.extractor_model.layers[-1].output]\n","            self.extractor_model.output_layers = [self.extractor_model.layers[-1]]\n","            #self.extractor_model.layers[-1].outbound_nodes = []\n","\n","    def extract(self, image_path):\n","        img = image.load_img(image_path, target_size=(299, 299))\n","        x = image.img_to_array(img)\n","        x = np.expand_dims(x, axis=0)\n","        x = preprocess_input(x)\n","\n","        # Get the prediction.\n","        features = self.extractor_model.predict(x)\n","        if self.weights is None:\n","            # For imagenet/default network:\n","            features = features[0]\n","        else:\n","            # For loaded network:\n","            features = features[0]\n","\n","        return features\n"]},{"cell_type":"code","execution_count":119,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 2/2333 [21:35<419:15:42, 647.51s/it]\n"," 74%|███████▍  | 1728/2333 [8:12:05<3:28:07, 20.64s/it]"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/Volumes/MY PASSPORT/Base_directory/Real/actors_08__talking_against_wall_1011.jpg'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-119-ba475ec632ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-118-dc25f636d958>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m   \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0m\u001b[1;32m    300\u001b[0m                         target_size=target_size, interpolation=interpolation)\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/MY PASSPORT/Base_directory/Real/actors_08__talking_against_wall_1011.jpg'"]}],"source":["import numpy as np\n","import os.path\n","from tqdm import tqdm\n","import os\n","\n","# Set defaults.\n","seq_length = 100\n","class_limit = 1  \n","# Get the dataset.\n","data = DataSet(seq_length=seq_length, class_limit=class_limit)\n","\n","# get the model.\n","extractor_model = Extractor(weights =\"effec\")\n","data_path = \"/Volumes/MY PASSPORT/Base_directory/\"\n","# Loop through data.\n","pbar = tqdm(total=len(data.data))\n","for i in range (1, len(data.data)):\n","    video = data.data[i]\n","    # Get the path to the sequence for this video.\n","    path = os.path.join(data_path, 'sequences', video[1] + '-' + str(seq_length) + \\\n","        '-features')  # numpy will auto-append .npy\n","    # Check if we already have it.\n","\n","    if os.path.isfile(path + '.npy'):\n","        pbar.update(1)\n","        continue\n","    else:\n","        os.makedirs(path,exist_ok=True)\n","\n","    # Get the frames for this video.\n","    frames = data.get_frames_for_sample(video)\n","    # Now downsample to just the ones we need.\n","    frames = data.rescale_list(frames, seq_length)\n","\n","    # Now loop through and extract features to build the sequence.\n","    sequence = []\n","    for img in frames:\n","        features = extractor_model.extract(img)\n","        sequence.append(features)\n","\n","    # Save the sequence.\n","    np.save(path, sequence)\n","\n","    pbar.update(1)\n","\n","pbar.close()\n"]},{"cell_type":"code","execution_count":106,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stderr","text":["a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_90612) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block3c_se_reduce_layer_call_and_return_conditional_losses_69567) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block5d_activation_layer_call_and_return_conditional_losses_73429) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_95523) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_75607) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_96001) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_72191) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_98071) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_102687) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_75067) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block5d_se_reduce_layer_call_and_return_conditional_losses_73476) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_70712) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6f_se_reduce_layer_call_and_return_conditional_losses_105505) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_100284) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6f_expand_activation_layer_call_and_return_conditional_losses_76405) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6e_activation_layer_call_and_return_conditional_losses_76053) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_102727) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block5e_expand_activation_layer_call_and_return_conditional_losses_101979) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block5e_activation_layer_call_and_return_conditional_losses_102122) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_105887) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block4e_activation_layer_call_and_return_conditional_losses_71698) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_70014) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block1b_activation_layer_call_and_return_conditional_losses_66948) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_99766) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6e_se_reduce_layer_call_and_return_conditional_losses_104940) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_76851) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block4e_activation_layer_call_and_return_conditional_losses_99344) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_103810) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block5e_se_reduce_layer_call_and_return_conditional_losses_73922) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block4d_activation_layer_call_and_return_conditional_losses_71252) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_97131) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_104375) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_75513) with ops with custom gradients. Will likely fail if a gradient is requested.\n","WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_74715) with ops with custom gradients. Will likely fail if a gradient is requested.\n","  0%|          | 0/2333 [06:46<?, ?it/s]\n","  0%|          | 0/2333 [06:00<?, ?it/s]\n","  0%|          | 0/2333 [05:43<?, ?it/s]\n"]}],"source":["model_2 = load_model(\"/Users/asmaaaly/Minerva/Minerva_Capstone/2.Models/Weights/results/EfficientNetB3\",compile=False)"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["model_2.compile(loss=loss,\n","                      optimizer=Adam(lr=0.0001), \n","                  metrics=['acc'])"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["model_2.outputs = [model_2.layers[-1].output]"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.core.Dense at 0x7f8071042130>"]},"metadata":{},"execution_count":114}],"source":["model_2.layers[-1]"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":117}],"source":["model_2.layers[-1].outbound_nodes "]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["from tensorflow.keras.layers import TimeDistributed\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from tensorflow.keras.layers import TimeDistributed\n","from tensorflow.keras.layers import (Conv2D, MaxPooling3D, Conv3D,\n","    MaxPooling2D)\n","from collections import deque\n","import sys"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[],"source":["\"\"\"\n","A collection of models we'll use to attempt to classify videos.\n","\"\"\"\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from tensorflow.keras.layers import TimeDistributed\n","from tensorflow.keras.layers import (Conv2D, MaxPooling3D, Conv3D,\n","    MaxPooling2D)\n","from collections import deque\n","import sys\n","\n","class ResearchModels():\n","    def __init__(self, nb_classes, model, seq_length,\n","                 saved_model=None, features_length=2304):\n","        \"\"\"\n","        `model` = one of:\n","            lstm\n","            lrcn\n","            mlp\n","            conv_3d\n","            c3d\n","        `nb_classes` = the number of classes to predict\n","        `seq_length` = the length of our video sequences\n","        `saved_model` = the path to a saved Keras model to load\n","        \"\"\"\n","\n","        # Set defaults.\n","        self.seq_length = seq_length\n","        self.load_model = load_model\n","        self.saved_model = saved_model\n","        self.nb_classes = nb_classes\n","        self.feature_queue = deque()\n","\n","        # Set the metrics. Only use top k if there's a need.\n","        metrics = ['accuracy']\n","        if self.nb_classes >= 10:\n","            metrics.append('top_k_categorical_accuracy')\n","\n","        # Get the appropriate model.\n","        if self.saved_model is not None:\n","            print(\"Loading model %s\" % self.saved_model)\n","            self.model = load_model(self.saved_model)\n","        elif model == 'gru':\n","            print(\"Loading GRU model.\")\n","            self.input_shape = (seq_length, features_length)\n","            self.model = self.gru()\n","        elif model == 'lrcn':\n","            print(\"Loading CNN-LSTM model.\")\n","            self.input_shape = (seq_length, 80, 80, 3)\n","            self.model = self.lrcn()\n","        elif model == 'mlp':\n","            print(\"Loading simple MLP.\")\n","            self.input_shape = (seq_length, features_length)\n","            self.model = self.mlp()\n","        elif model == 'conv_3d':\n","            print(\"Loading Conv3D\")\n","            self.input_shape = (seq_length, 80, 80, 3)\n","            self.model = self.conv_3d()\n","        elif model == 'c3d':\n","            print(\"Loading C3D\")\n","            self.input_shape = (seq_length, 80, 80, 3)\n","            self.model = self.c3d()\n","        else:\n","            print(\"Unknown network.\")\n","            sys.exit()\n","\n","        # Now compile the network.\n","        optimizer = Adam(lr=0.001)\n","        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n","                           metrics=metrics)\n","\n","        print(self.model.summary())\n","\n","    def gru(self):\n","        \"\"\"Build a simple LSTM network. We pass the extracted features from\n","        our CNN to this model predomenently.\n","        # Model.\n","        \"\"\"\n","        model = Sequential()\n","        model.add(GRU(2304, return_sequences=False,\n","                       input_shape=self.input_shape,\n","                       dropout=0.1))\n","        model.add(Dense(512, activation='relu'))\n","        model.add(Dropout(0.1))\n","        model.add(Dense(self.nb_classes, activation='softmax'))\n","\n","        return model\n","        \"\"\"\n","        # Create our convnet with (112, 112, 3) input shape        \n","        # then create our final model\n","        model = Sequential()\n","        # add the convnet with (5, 112, 112, 3) shape\n","        # here, you can also use GRU or LSTM\n","        model.add(GRU(64))\n","        # and finally, we make a decision network\n","        model.add(Dense(2304, activation='relu'))\n","        model.add(Dropout(.5))\n","        model.add(Dense(512, activation='relu'))\n","        model.add(Dropout(.5))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dropout(.5))\n","        model.add(Dense(64, activation='relu'))\n","        model.add(Dense(self.nb_classes, activation='softmax'))\n","        return model\n","        \"\"\"\n","\n","    def lrcn(self):\n","        \"\"\"Build a CNN into RNN.\n","        Starting version from:\n","            https://github.com/udacity/self-driving-car/blob/master/\n","                steering-models/community-models/chauffeur/models.py\n","\n","        Heavily influenced by VGG-16:\n","            https://arxiv.org/abs/1409.1556\n","\n","        Also known as an LRCN:\n","            https://arxiv.org/pdf/1411.4389.pdf\n","        \"\"\"\n","        def add_default_block(model, kernel_filters, init, reg_lambda):\n","\n","            # conv\n","            model.add(TimeDistributed(Conv2D(kernel_filters, (3, 3), padding='same',\n","                                             kernel_initializer=init, kernel_regularizer=L2_reg(l=reg_lambda))))\n","            model.add(TimeDistributed(BatchNormalization()))\n","            model.add(TimeDistributed(Activation('relu')))\n","            # conv\n","            model.add(TimeDistributed(Conv2D(kernel_filters, (3, 3), padding='same',\n","                                             kernel_initializer=init, kernel_regularizer=L2_reg(l=reg_lambda))))\n","            model.add(TimeDistributed(BatchNormalization()))\n","            model.add(TimeDistributed(Activation('relu')))\n","            # max pool\n","            model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","\n","            return model\n","\n","        initialiser = 'glorot_uniform'\n","        reg_lambda  = 0.001\n","\n","        model = Sequential()\n","\n","        # first (non-default) block\n","        model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2), padding='same',\n","                                         kernel_initializer=initialiser, kernel_regularizer=L2_reg(l=reg_lambda)),\n","                                  input_shape=self.input_shape))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Activation('relu')))\n","        model.add(TimeDistributed(Conv2D(32, (3,3), kernel_initializer=initialiser, kernel_regularizer=L2_reg(l=reg_lambda))))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Activation('relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","\n","        # 2nd-5th (default) blocks\n","        model = add_default_block(model, 64,  init=initialiser, reg_lambda=reg_lambda)\n","        model = add_default_block(model, 128, init=initialiser, reg_lambda=reg_lambda)\n","        model = add_default_block(model, 256, init=initialiser, reg_lambda=reg_lambda)\n","        model = add_default_block(model, 512, init=initialiser, reg_lambda=reg_lambda)\n","\n","        # LSTM output head\n","        model.add(TimeDistributed(Flatten()))\n","        model.add(LSTM(256, return_sequences=False, dropout=0.5))\n","        model.add(Dense(self.nb_classes, activation='softmax'))\n","\n","        return model\n","\n","    def mlp(self):\n","        \"\"\"Build a simple MLP. It uses extracted features as the input\n","        because of the otherwise too-high dimensionality.\"\"\"\n","        # Model.\n","        model = Sequential()\n","        model.add(Flatten(input_shape=self.input_shape))\n","        model.add(Dense(512))\n","        model.add(Dropout(0.1))\n","        model.add(Dense(512))\n","        model.add(Dropout(0.1))\n","        model.add(Dense(self.nb_classes, activation='softmax'))\n","\n","        return model\n","\n","    def conv_3d(self):\n","        \"\"\"\n","        Build a 3D convolutional network, based loosely on C3D.\n","            https://arxiv.org/pdf/1412.0767.pdf\n","        \"\"\"\n","        # Model.\n","        model = Sequential()\n","        model.add(Conv3D(\n","            32, (3,3,3), activation='relu', input_shape=self.input_shape\n","        ))\n","        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n","        model.add(Conv3D(64, (3,3,3), activation='relu'))\n","        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n","        model.add(Conv3D(128, (3,3,3), activation='relu'))\n","        model.add(Conv3D(128, (3,3,3), activation='relu'))\n","        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n","        model.add(Conv3D(256, (2,2,2), activation='relu'))\n","        model.add(Conv3D(256, (2,2,2), activation='relu'))\n","        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n","\n","        model.add(Flatten())\n","        model.add(Dense(1024))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(1024))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(self.nb_classes, activation='softmax'))\n","\n","        return model\n","\n","    def c3d(self):\n","        \"\"\"\n","        Build a 3D convolutional network, aka C3D.\n","            https://arxiv.org/pdf/1412.0767.pdf\n","\n","        With thanks:\n","            https://gist.github.com/albertomontesg/d8b21a179c1e6cca0480ebdf292c34d2\n","        \"\"\"\n","        model = Sequential()\n","        # 1st layer group\n","        model.add(Conv3D(64, 3, 3, 3, activation='relu',\n","                         border_mode='same', name='conv1',\n","                         subsample=(1, 1, 1),\n","                         input_shape=self.input_shape))\n","        model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2),\n","                               border_mode='valid', name='pool1'))\n","        # 2nd layer group\n","        model.add(Conv3D(128, 3, 3, 3, activation='relu',\n","                         border_mode='same', name='conv2',\n","                         subsample=(1, 1, 1)))\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n","                               border_mode='valid', name='pool2'))\n","        # 3rd layer group\n","        model.add(Conv3D(256, 3, 3, 3, activation='relu',\n","                         border_mode='same', name='conv3a',\n","                         subsample=(1, 1, 1)))\n","        model.add(Conv3D(256, 3, 3, 3, activation='relu',\n","                         border_mode='same', name='conv3b',\n","                         subsample=(1, 1, 1)))\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n","                               border_mode='valid', name='pool3'))\n","        # 4th layer group\n","        model.add(Conv3D(512, 3, 3, 3, activation='relu',\n","                         border_mode='same', name='conv4a',\n","                         subsample=(1, 1, 1)))\n","        model.add(Conv3D(512, 3, 3, 3, activation='relu',\n","                         border_mode='same', name='conv4b',\n","                         subsample=(1, 1, 1)))\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n","                               border_mode='valid', name='pool4'))\n","\n","        # 5th layer group\n","        model.add(Conv3D(512, 3, 3, 3, activation='relu',\n","                         border_mode='same', name='conv5a',\n","                         subsample=(1, 1, 1)))\n","        model.add(Conv3D(512, 3, 3, 3, activation='relu',\n","                         border_mode='same', name='conv5b',\n","                         subsample=(1, 1, 1)))\n","        model.add(ZeroPadding3D(padding=(0, 1, 1)))\n","        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2),\n","                               border_mode='valid', name='pool5'))\n","        model.add(Flatten())\n","\n","        # FC layers group\n","        model.add(Dense(4096, activation='relu', name='fc6'))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(4096, activation='relu', name='fc7'))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(self.nb_classes, activation='softmax'))\n","\n","        return model\n"]},{"cell_type":"code","execution_count":103,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading GRU model.\n","Model: \"sequential_11\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_10 (GRU)                 (None, 2304)              31864320  \n","_________________________________________________________________\n","dense_19 (Dense)             (None, 512)               1180160   \n","_________________________________________________________________\n","dropout_9 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_20 (Dense)             (None, 2)                 1026      \n","=================================================================\n","Total params: 33,045,506\n","Trainable params: 33,045,506\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Creating train generator with 1398 samples.\n","Epoch 1/30\n","139/139 [==============================] - ETA: 0s - loss: 1.2900 - accuracy: 0.5340Creating valid generator with 467 samples.\n","139/139 [==============================] - 947s 7s/step - loss: 1.2871 - accuracy: 0.5340 - val_loss: 0.7036 - val_accuracy: 0.4761\n","\n","Epoch 00001: accuracy improved from -inf to 0.52590, saving model to weights-GRU.h5\n","Epoch 2/30\n","139/139 [==============================] - 869s 6s/step - loss: 0.6923 - accuracy: 0.5247 - val_loss: 0.7357 - val_accuracy: 0.4891\n","\n","Epoch 00002: accuracy improved from 0.52590 to 0.53813, saving model to weights-GRU.h5\n","Epoch 3/30\n","139/139 [==============================] - 813s 6s/step - loss: 0.6964 - accuracy: 0.5210 - val_loss: 0.6900 - val_accuracy: 0.5348\n","\n","Epoch 00003: accuracy did not improve from 0.53813\n","Epoch 4/30\n","139/139 [==============================] - 1020s 7s/step - loss: 0.6718 - accuracy: 0.5780 - val_loss: 0.6935 - val_accuracy: 0.5000\n","\n","Epoch 00004: accuracy improved from 0.53813 to 0.56763, saving model to weights-GRU.h5\n","Epoch 5/30\n","139/139 [==============================] - 931s 7s/step - loss: 0.6948 - accuracy: 0.5108 - val_loss: 0.6980 - val_accuracy: 0.5630\n","\n","Epoch 00005: accuracy did not improve from 0.56763\n","Epoch 6/30\n","139/139 [==============================] - 811s 6s/step - loss: 0.6721 - accuracy: 0.6202 - val_loss: 0.6971 - val_accuracy: 0.5587\n","\n","Epoch 00006: accuracy improved from 0.56763 to 0.60647, saving model to weights-GRU.h5\n","Epoch 7/30\n","139/139 [==============================] - 793s 6s/step - loss: 0.6580 - accuracy: 0.6214 - val_loss: 0.6845 - val_accuracy: 0.5783\n","\n","Epoch 00007: accuracy did not improve from 0.60647\n","Epoch 8/30\n","139/139 [==============================] - 803s 6s/step - loss: 0.6607 - accuracy: 0.6060 - val_loss: 0.7002 - val_accuracy: 0.6043\n","\n","Epoch 00008: accuracy improved from 0.60647 to 0.62662, saving model to weights-GRU.h5\n","Epoch 9/30\n","139/139 [==============================] - 871s 6s/step - loss: 0.6427 - accuracy: 0.6310 - val_loss: 0.6817 - val_accuracy: 0.5848\n","\n","Epoch 00009: accuracy did not improve from 0.62662\n","Epoch 10/30\n","139/139 [==============================] - 813s 6s/step - loss: 0.6237 - accuracy: 0.6525 - val_loss: 0.7216 - val_accuracy: 0.5804\n","\n","Epoch 00010: accuracy improved from 0.62662 to 0.63885, saving model to weights-GRU.h5\n","Epoch 11/30\n","139/139 [==============================] - 803s 6s/step - loss: 0.5827 - accuracy: 0.6891 - val_loss: 0.6907 - val_accuracy: 0.5696\n","\n","Epoch 00011: accuracy improved from 0.63885 to 0.67914, saving model to weights-GRU.h5\n","Epoch 12/30\n"," 44/139 [========>.....................] - ETA: 8:30 - loss: 0.6667 - accuracy: 0.6446"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-103-fcfc9d2282e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid model. See train.py for options.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m train(data_type, seq_length, model, saved_model=saved_model,\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mclass_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         load_to_memory=load_to_memory, batch_size=batch_size, nb_epoch=nb_epoch)\n","\u001b[0;32m<ipython-input-103-fcfc9d2282e6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_type, seq_length, model, saved_model, class_limit, image_shape, load_to_memory, batch_size, nb_epoch)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Use fit generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         rm.model.fit_generator(\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                   \u001b[0;34m'will be removed in a future version. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[0;32m-> 1847\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   1848\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\"\"\"\n","Train our RNN on extracted features or images.\n","\"\"\"\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n","import time\n","import os.path\n","\n","def train(data_type, seq_length, model, saved_model=None,\n","          class_limit=None, image_shape=None,\n","          load_to_memory=False, batch_size=32, nb_epoch=100):\n","    # Helper: Save the model.\n","    mode_version = \"GRU\"\n","    checkpointer = ModelCheckpoint(f'weights-{mode_version}.h5', monitor='accuracy',\n","    save_best_only=True, save_weights_only=True, verbose=1)\n","\n","    # Helper: TensorBoard\n","    #tb = TensorBoard(log_dir=os.path.join('data', 'logs', model))\n","\n","    # Helper: Stop when we stop learning.\n","    early_stopper = EarlyStopping(patience=5)\n","\n","    # Helper: Save results.\n","    timestamp = time.time()\n","    csv_logger = CSVLogger(os.path.join('data', 'logs', model + '-' + 'training-' + \\\n","        str(timestamp) + '.log'))\n","\n","    # Get the data and process it.\n","    if image_shape is None:\n","        data = DataSet(\n","            seq_length=seq_length,\n","            class_limit=class_limit\n","        )\n","    else:\n","        data = DataSet(\n","            seq_length=seq_length,\n","            class_limit=class_limit,\n","            image_shape=image_shape\n","        )\n","\n","    # Get samples per epoch.\n","    # Multiply by 0.7 to attempt to guess how much of data.data is the train set.\n","    steps_per_epoch = 1398 // batch_size\n","\n","    if load_to_memory:\n","        # Get data.\n","        X, y = data.get_all_sequences_in_memory('train', data_type)\n","        X_test, y_test = data.get_all_sequences_in_memory('test', data_type)\n","    else:\n","        # Get generators.\n","        generator = data.frame_generator(batch_size, 'train', data_type)\n","        val_generator = data.frame_generator(batch_size, 'valid', data_type)\n","\n","    # Get the model.\n","    rm = ResearchModels(len(data.classes), model, seq_length, saved_model)\n","\n","    # Fit!\n","    if load_to_memory:\n","        # Use standard fit.\n","        rm.model.fit(\n","            X,\n","            y,\n","            batch_size=batch_size,\n","            validation_data=(X_test, y_test),\n","            verbose=1,\n","            callbacks=[early_stopper, csv_logger],\n","            epochs=nb_epoch)\n","    else:\n","        # Use fit generator.\n","        rm.model.fit_generator(\n","            generator=generator,\n","            steps_per_epoch=steps_per_epoch,\n","            epochs=nb_epoch,\n","            verbose=1,\n","            callbacks=[checkpointer],\n","            validation_data=val_generator,\n","            validation_steps=46,\n","            workers=4)\n","\n","model = 'gru'\n","saved_model = None  # None or weights file\n","class_limit = 2  # int, can be 1-101 or None\n","seq_length = 100\n","load_to_memory = False  # pre-load the sequences into memory\n","batch_size = 10\n","nb_epoch = 30\n","\n","# Chose images or features and image shape based on network.\n","if model in ['conv_3d', 'c3d', 'lrcn']:\n","    data_type = 'images'\n","    image_shape = (80, 80, 3)\n","elif model in ['gru', 'mlp']:\n","    data_type = 'features'\n","    image_shape = None\n","else:\n","    raise ValueError(\"Invalid model. See train.py for options.\")\n","\n","train(data_type, seq_length, model, saved_model=saved_model,\n","        class_limit=class_limit, image_shape=image_shape,\n","        load_to_memory=load_to_memory, batch_size=batch_size, nb_epoch=nb_epoch)\n"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = Sequential()\n","# add the convnet with (5, 112, 112, 3) shape\n","# here, you can also use GRU or LSTM\n","model.add(GRU(2048, return_sequences=False,\n","                       input_shape=(40,2048),\n","                       dropout=0.3))# and finally, we make a decision network\n","model.add(Dropout(.5))\n","model.add(Dense(512, activation='relu'))\n","model.add(Dropout(.5))\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(.5))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(2, activation='softmax'))\n","model.summary()\n","        "]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[],"source":["import pandas as pd\n","data = pd.read_csv(\"/Users/asmaaaly/Minerva/Minerva_Capstone/1.Preprocess Data/only_videos_with_labels.csv\")"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[],"source":["train_pd = data[data[\"Split_type\"]==\"Train\"]"]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[],"source":["validation_pd = data[data[\"Split_type\"]==\"Valid\"]"]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[],"source":["test_pd = data[data[\"Split_type\"]==\"Test\"]"]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["467"]},"metadata":{},"execution_count":158}],"source":["len(validation_pd)"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["46"]},"metadata":{},"execution_count":165}],"source":["467//10"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["test = np.load(\"/Volumes/MY PASSPORT/Base_directory/sequences/415Deepfakes-100-features.npy\")"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100, 2304)"]},"metadata":{},"execution_count":92}],"source":["np.shape(test)"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":102}],"source":["len(data.classes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def gru(self):\n","    \"\"\"Build a simple LSTM network. We pass the extracted features from\n","    our CNN to this model predomenently.\n","    # Model.\n","    \"\"\"\n","    model = Sequential()\n","    model.add(GRU(2304, return_sequences=False,\n","                    input_shape=self.input_shape,\n","                    dropout=0.1))\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dropout(0.1))\n","    model.add(Dense(self.nb_classes, activation='softmax'))\n","\n","    return model"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["import pandas as pd\n","data_full = pd.read_csv(\"/Users/asmaaaly/Minerva/Minerva_Capstone/2.Models/merged_file_all_data.csv\", index_col=0)\n","only_dfdc = data_full[data_full.Dataset.isin(['actors', 'deepfakedetection'])]\n","only_unique = only_dfdc.copy()\n","only_unique = only_unique.drop_duplicates(subset=['Video_ID'])\n"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["only_unique = only_unique.drop(\"Split Type\", axis =1 )"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0    359\n","1.0    158\n","Name: Label, dtype: int64"]},"metadata":{},"execution_count":60}],"source":["only_unique[\"Label\"].value_counts()"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["df_0 = only_unique[only_unique['Label'] == 0].sample(158, random_state = 101)\n","df_1 = only_unique[only_unique['Label'] == 1].sample(158, random_state = 101)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["import numpy as np\n","df_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n","# shuffle\n","#print to see if the data is balanced \n","df_data['Label'].value_counts()\n","y = df_data['Label']\n","df_train, df_test = train_test_split(df_data, test_size=0.20, random_state=101, stratify=y)\n","\n","y_train = df_train['Label']\n","df_train, df_valid = train_test_split(df_train, test_size=0.25, random_state=101,stratify=y_train)\n","df_train[\"Split Type\"] = np.asarray(['Train'] * len(df_train))\n","df_valid[\"Split Type\"] = np.asarray(['Valid'] * len(df_valid))\n","df_test[\"Split Type\"] = np.asarray(['Test'] * len(df_test))\n","Processed_frames = [df_train, df_valid,df_test]\n","df_data_used = pd.concat(Processed_frames)\n"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["df_data_used = df_data_used[~(df_data_used['Frame_ID_x'] <= 100)] "]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["df_data_used.to_csv(\"df_data_used.csv\", header = False)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0    158\n","1.0    158\n","Name: Label, dtype: int64"]},"metadata":{},"execution_count":65}],"source":["df_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n","# shuffle\n","#print to see if the data is balanced \n","df_data['Label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.8.0 64-bit","metadata":{"interpreter":{"hash":"4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"}}},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.8.0-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}